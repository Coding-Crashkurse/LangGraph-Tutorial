{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Agent Swarm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from swarm import Swarm, Agent\n",
    "\n",
    "client = Swarm()\n",
    "\n",
    "def get_weather(city: str):\n",
    "    return f\"The weather in {city} is always 30°C.\"\n",
    "\n",
    "def get_air_quality(city: str):\n",
    "    return f\"The air quality in {city} is 'Good' with an AQI of 42.\"\n",
    "\n",
    "def route_request(context_variables):\n",
    "    user_message = context_variables.get(\"last_user_message\", \"\")\n",
    "    if \"weather\" in user_message.lower():\n",
    "        return weather_agent\n",
    "    elif \"air quality\" in user_message.lower():\n",
    "        return air_quality_agent\n",
    "    else:\n",
    "        return \"I'm sorry, I don't understand your request.\"\n",
    "\n",
    "weather_agent = Agent(\n",
    "    name=\"Weather Agent\",\n",
    "    instructions=\"You are a weather assistant. Provide the weather information when asked.\",\n",
    "    functions=[get_weather]\n",
    ")\n",
    "\n",
    "air_quality_agent = Agent(\n",
    "    name=\"Air Quality Agent\",\n",
    "    instructions=\"You are an air quality assistant. Provide the air quality information when asked.\",\n",
    "    functions=[get_air_quality]\n",
    ")\n",
    "\n",
    "supervisor_agent = Agent(\n",
    "    name=\"Supervisor Agent\",\n",
    "    instructions=\"You are the supervisor. Determine whether the user's request is about weather or air quality, and transfer them to the appropriate agent by calling the 'route_request' function.\",\n",
    "    functions=[route_request]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_message = \"What's the weather like in Paris?\"\n",
    "context_variables = {\"last_user_message\": user_message}\n",
    "\n",
    "response = client.run(\n",
    "    agent=supervisor_agent,\n",
    "    messages=[{\"role\": \"user\", \"content\": user_message}],\n",
    "    context_variables=context_variables\n",
    ")\n",
    "\n",
    "print(response.messages[-1][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_message = \"How's the air quality in Tokyo?\"\n",
    "context_variables = {\"last_user_message\": user_message}\n",
    "\n",
    "response = client.run(\n",
    "    agent=supervisor_agent,\n",
    "    messages=[{\"role\": \"user\", \"content\": user_message}],\n",
    "    context_variables=context_variables\n",
    ")\n",
    "\n",
    "print(response.messages[-1][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Same with LangGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, List\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI  # Corrected import\n",
    "from langchain.schema import HumanMessage, AIMessage, SystemMessage, BaseMessage\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.tools import tool\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "# Define the tools using the @tool decorator\n",
    "@tool\n",
    "def get_weather(city: str) -> str:\n",
    "    \"\"\"Get the weather information for a given city.\"\"\"\n",
    "    return f\"The weather in {city} is always 30°C.\"\n",
    "\n",
    "@tool\n",
    "def get_air_quality(city: str) -> str:\n",
    "    \"\"\"Get the air quality information for a given city.\"\"\"\n",
    "    return f\"The air quality in {city} is 'Good' with an AQI of 42.\"\n",
    "\n",
    "@tool\n",
    "def default_answer() -> str:\n",
    "    \"\"\"Provide a default response when unable to answer.\"\"\"\n",
    "    return \"I'm sorry, I can't answer that.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[List[BaseMessage], add_messages]\n",
    "    on_topic: str\n",
    "    classification: str\n",
    "    system_message: SystemMessage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_weather_tools = [get_weather]\n",
    "get_air_quality_tools = [get_air_quality]\n",
    "default_answer_tools = [default_answer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "supervisor_llm = ChatOpenAI(temperature=0)\n",
    "weather_llm = ChatOpenAI(temperature=0).bind_tools(get_weather_tools)\n",
    "air_quality_llm = ChatOpenAI(temperature=0).bind_tools(get_air_quality_tools)\n",
    "off_topic_llm = ChatOpenAI(temperature=0).bind_tools(default_answer_tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import AIMessage, SystemMessage\n",
    "from typing import Dict, Any\n",
    "\n",
    "\n",
    "# Function to invoke the weather model\n",
    "def call_weather_model(state: AgentState) -> Dict[str, Any]:\n",
    "    messages = state[\"messages\"]\n",
    "    system_message = SystemMessage(\n",
    "        content=\"You are WeatherBot. Answer the user's weather-related questions only in French.\"\n",
    "    )\n",
    "    conversation = [system_message] + messages\n",
    "    print(\"Weather conversation:\", conversation)  # Debug statement\n",
    "    response = weather_llm.invoke(conversation)\n",
    "    print(\"Weather response:\", response)  # Debug statement\n",
    "    return {\"messages\": state[\"messages\"] + [response]}  # Return the updated messages\n",
    "\n",
    "# Function to invoke the air quality model\n",
    "def call_air_quality_model(state: AgentState) -> Dict[str, Any]:\n",
    "    messages = state[\"messages\"]\n",
    "    system_message = SystemMessage(\n",
    "        content=\"You are AirQualityBot. Provide air quality information in a very formal and polite manner.\"\n",
    "    )\n",
    "    conversation = [system_message] + messages\n",
    "    print(\"Air Quality conversation:\", conversation)  # Debug statement\n",
    "    response = air_quality_llm.invoke(conversation)\n",
    "    print(\"Air Quality response:\", response)  # Debug statement\n",
    "    return {\"messages\": state[\"messages\"] + [response]}  # Return the updated messages\n",
    "\n",
    "# Function to invoke the off-topic model\n",
    "def call_off_topic_model(state: AgentState) -> Dict[str, Any]:\n",
    "    messages = state[\"messages\"]\n",
    "    system_message = SystemMessage(\n",
    "        content=\"You are OffTopicBot. Apologize to the user and explain that you cannot help with their request, but do so in a friendly tone.\"\n",
    "    )\n",
    "    conversation = [system_message] + messages\n",
    "    print(\"Off-Topic conversation:\", conversation)  # Debug statement\n",
    "    response = off_topic_llm.invoke(conversation)\n",
    "    print(\"Off-Topic response:\", response)  # Debug statement\n",
    "    return {\"messages\": state[\"messages\"] + [response]}  # Return the updated messages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def should_continue_weather(state: AgentState):\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    if hasattr(last_message, 'tool_calls') and last_message.tool_calls:\n",
    "        return \"weather_tools\"\n",
    "    return END\n",
    "\n",
    "def should_continue_air_quality(state: AgentState):\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    if hasattr(last_message, 'tool_calls') and last_message.tool_calls:\n",
    "        return \"air_quality_tools\"\n",
    "    return END\n",
    "\n",
    "def should_continue_off_topic(state: AgentState):\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    if hasattr(last_message, 'tool_calls') and last_message.tool_calls:\n",
    "        return \"off_topic_tools\"\n",
    "    return END\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# Define the model for the supervisor's decision\n",
    "class SupervisorDecision(BaseModel):\n",
    "    \"\"\"Decision made by the supervisor agent.\"\"\"\n",
    "    classification: str = Field(description=\"Classify the message as 'weather', 'air_quality', or 'other'\")\n",
    "\n",
    "# Function to invoke the supervisor model\n",
    "def call_supervisor_model(state: AgentState) -> AgentState:\n",
    "\n",
    "    print(state)\n",
    "\n",
    "    messages = state['messages']\n",
    "    last_message = messages[-1].content if messages else ''\n",
    "\n",
    "    # Define the system prompt\n",
    "    system_prompt = \"\"\"You are a supervisor agent that decides whether the user's message is on topic and classifies it.\n",
    "\n",
    "    Analyze the user's message and decide:\n",
    "\n",
    "    Classify it as 'weather', 'air_quality', or 'other' if on topic.\n",
    "\n",
    "    Provide your decision in the following structured format:\n",
    "        \"classification\": \"weather\" or \"air_quality\" or \"other\"\n",
    "    \"\"\"\n",
    "\n",
    "    # Create the prompt template with system and user message\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [(\"system\", system_prompt),\n",
    "         (\"human\", \"User Message:\\n\\n{user_message}\")]\n",
    "    )\n",
    "\n",
    "    # LLM with structured output for classification\n",
    "    structured_supervisor_llm = supervisor_llm.with_structured_output(SupervisorDecision)\n",
    "    evaluator = prompt | structured_supervisor_llm\n",
    "\n",
    "    # Invoke the LLM with the last user message\n",
    "    result = evaluator.invoke({\"user_message\": last_message})\n",
    "\n",
    "    # Store classification result in state\n",
    "    classification = result.classification\n",
    "    state['classification'] = classification\n",
    "\n",
    "    return state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "weather_tool_node = ToolNode(tools=[get_weather])\n",
    "air_quality_tool_node = ToolNode(tools=[get_air_quality])\n",
    "off_topic_tool_node = ToolNode(tools=[default_answer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def supervisor_router(state: AgentState) -> str:\n",
    "    classification = state.get('classification', '')\n",
    "    if classification == 'weather':\n",
    "        return 'weather_model'\n",
    "    elif classification == 'air_quality':\n",
    "        return 'air_quality_model'\n",
    "    else:\n",
    "        return 'off_topic_model'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def should_continue_weather(state: AgentState):\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    if hasattr(last_message, 'tool_calls') and last_message.tool_calls:\n",
    "        return \"weather_tools\"\n",
    "    return END\n",
    "\n",
    "def should_continue_air_quality(state: AgentState):\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    if hasattr(last_message, 'tool_calls') and last_message.tool_calls:\n",
    "        return \"air_quality_tools\"\n",
    "    return END\n",
    "\n",
    "def should_continue_off_topic(state: AgentState):\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    if hasattr(last_message, 'tool_calls') and last_message.tool_calls:\n",
    "        return \"off_topic_tools\"\n",
    "    return END\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the graph\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "# Supervisor node\n",
    "workflow.add_node(\"supervisor_agent\", call_supervisor_model)\n",
    "\n",
    "# Weather agent nodes\n",
    "workflow.add_node(\"weather_model\", call_weather_model)\n",
    "workflow.add_node(\"weather_tools\", weather_tool_node)\n",
    "\n",
    "# Air quality agent nodes\n",
    "workflow.add_node(\"air_quality_model\", call_air_quality_model)\n",
    "workflow.add_node(\"air_quality_tools\", air_quality_tool_node)\n",
    "\n",
    "# Off-topic agent nodes\n",
    "workflow.add_node(\"off_topic_model\", call_off_topic_model)\n",
    "workflow.add_node(\"off_topic_tools\", off_topic_tool_node)\n",
    "\n",
    "# Edges\n",
    "workflow.add_edge(START, \"supervisor_agent\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"supervisor_agent\", supervisor_router,\n",
    "    [\"weather_model\", \"air_quality_model\", \"off_topic_model\"]\n",
    ")\n",
    "\n",
    "# Weather agent workflow\n",
    "workflow.add_conditional_edges(\"weather_model\", should_continue_weather, [\"weather_tools\", END])\n",
    "workflow.add_edge(\"weather_tools\", \"weather_model\")\n",
    "\n",
    "# Air quality agent workflow\n",
    "workflow.add_conditional_edges(\"air_quality_model\", should_continue_air_quality, [\"air_quality_tools\", END])\n",
    "workflow.add_edge(\"air_quality_tools\", \"air_quality_model\")\n",
    "\n",
    "# Off-topic agent workflow\n",
    "workflow.add_conditional_edges(\"off_topic_model\", should_continue_off_topic, [\"off_topic_tools\", END])\n",
    "workflow.add_edge(\"off_topic_tools\", \"off_topic_model\")\n",
    "\n",
    "# Compile the workflow\n",
    "app = workflow.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(app.get_graph().draw_mermaid_png()))\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import HumanMessage\n",
    "\n",
    "app.invoke({\"messages\": [HumanMessage(content=\"How is the weather in Munich?\")]})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
