{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af480eb1",
   "metadata": {},
   "source": [
    "# Responses API Tutorial: From Introduction to Summary\n",
    "\n",
    "Below is a comprehensive tutorial and set of examples demonstrating:\n",
    "\n",
    "1. **How the Responses API works and how it differs from Chat Completions** (particularly around stateful vs. stateless usage).\n",
    "2. **Examples** of multi-turn conversation (using `previous_response_id` for stateful flows) and built-in tools like web search and file search.\n",
    "3. **How to disable storage** (`store: false`) if you **do not** want your conversation state to persist on OpenAI’s servers—effectively making it stateless.\n",
    "\n",
    "---\n",
    "## 1. Chat Completions (Stateless) vs. Responses (Stateful)\n",
    "\n",
    "- **Chat Completions**:\n",
    "  - Typically stateless: each new request must supply the entire conversation history in `messages`.\n",
    "  - Stored by default only for new accounts; can be disabled.\n",
    "\n",
    "- **Responses**:\n",
    "  - By default, **stateful**: each response has its own `id`. You can pass `previous_response_id` in subsequent calls, and the system automatically includes the prior context.\n",
    "  - Provides built-in **tools** (web search, file search, etc.) that the model can call if relevant.\n",
    "  - **Stored** by default. If you want ephemeral usage, set `store: false`.\n",
    "\n",
    "When you get a response back from the Responses API, the returned object differs slightly from Chat Completions:\n",
    "\n",
    "- Instead of a simple list of message choices, you receive a typed `response` object with top-level fields (e.g. `id`, `output`, `usage`, etc.).\n",
    "- To continue a conversation, pass `previous_response_id` to the next request.\n",
    "- If you do **not** want it stored, set `store: false`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ba103f",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Multi-Turn Flow (Stateful) Example\n",
    "\n",
    "Using `previous_response_id` means the Responses API will store and automatically incorporate the entire conversation. Here’s a simple demonstration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc2fb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f729187f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "\n",
    "resp1 = client.responses.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    input=\"Hello there! You're a helpful math tutor. Could you help me with a question? What's 2 + 2?\"\n",
    ")\n",
    "print(\"First response:\\n\", resp1.output_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b8116f",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp2 = client.responses.create(\n",
    "    model=\"gpt-4o\",\n",
    "    input=\"Sure. How you you come to this conclusion?\",\n",
    "    previous_response_id=resp1.id\n",
    ")\n",
    "print(\"\\nSecond response:\\n\", resp2.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891a378a",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Using Built-In Tools\n",
    "\n",
    "### 3.1 Web Search\n",
    "Allows the model to gather recent info from the internet if relevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2857562e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage of the built-in web_search tool\n",
    "r1 = client.responses.create(\n",
    "    model=\"gpt-4o\",\n",
    "    input=\"Please find recent positive headlines about quantum computing.\",\n",
    "    tools=[{\"type\": \"web_search\"}]  # enabling built-in web search\n",
    ")\n",
    "print(r1.output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a4f76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue the conversation referencing previous response\n",
    "r2 = client.responses.create(\n",
    "    model=\"gpt-4o\",\n",
    "    input=\"Interesting! Summarize the second article.\",\n",
    "    previous_response_id=r1.id\n",
    ")\n",
    "print(\"\\nFollow-up:\\n\", r2.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599a206e",
   "metadata": {},
   "source": [
    "### 3.2 File Upload + File Search\n",
    "\n",
    "Below is the corrected snippet showing how to:\n",
    "1. **Upload** a local PDF (e.g., `dragon_book.pdf`).\n",
    "2. **Create** a vector store from that file.\n",
    "3. **Use** `file_search` in the Responses API to reference it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc24105",
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_resp = client.files.create(\n",
    "    file=open(\"dragon_book.txt\", \"rb\"),\n",
    "    purpose=\"user_data\"\n",
    ")\n",
    "file_id = upload_resp.id\n",
    "print(\"Uploaded file ID:\", file_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2b122e",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.files.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749870bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "vstore_resp = client.vector_stores.create(\n",
    "    name=\"DragonData\",\n",
    "    file_ids=[file_id]\n",
    ")\n",
    "vstore_id = vstore_resp.id\n",
    "print(\"Vector store ID:\", vstore_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dada70c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp1 = client.responses.create(\n",
    "    model=\"gpt-4o\",\n",
    "    tools=[{\n",
    "        \"type\": \"file_search\",\n",
    "        \"vector_store_ids\": [vstore_id],\n",
    "        \"max_num_results\": 3\n",
    "    }],\n",
    "    input=\"What Information do you have about red dragons?\"\n",
    ")\n",
    "print(resp1.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7d1705",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Disable Storage (Stateless Mode)\n",
    "\n",
    "Although the Responses API is **stateful** by default, you can make calls **not** store any conversation by setting `store=False`. Then `previous_response_id` won’t work, because no data is retained on OpenAI’s servers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553d63e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An ephemeral request that won't be stored\n",
    "ephemeral_resp = client.responses.create(\n",
    "    model=\"gpt-4o\",\n",
    "    input=\"Hello, let's do a single-turn question about geometry.\",\n",
    "    store=False  # ephemeral usage\n",
    ")\n",
    "print(ephemeral_resp.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366df08f",
   "metadata": {},
   "source": [
    "### LangChain Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf04310",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", use_responses_api=True)\n",
    "\n",
    "\n",
    "tool = {\"type\": \"web_search_preview\"}\n",
    "\n",
    "\n",
    "llm_with_tools = llm.bind_tools([tool])\n",
    "\n",
    "\n",
    "response = llm_with_tools.invoke(input=\"What was a positive news story from today?\")\n",
    "\n",
    "print(\"Text content:\", response.content)\n",
    "print(\"Tool calls:\", response.tool_calls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d14f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm_stateful = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    use_responses_api=True,\n",
    ")\n",
    "\n",
    "respA = llm_stateful.invoke(\"Hi, I'm Bob. Please remember my name.\")\n",
    "print(\"Response A:\", respA.content)\n",
    "print(\"A's ID:\", respA.response_metadata[\"id\"])\n",
    "\n",
    "respB = llm_stateful.invoke(\n",
    "    \"What is my name?\",\n",
    "    previous_response_id=respA.response_metadata[\"id\"]\n",
    ")\n",
    "print(\"Response B:\", respB.content)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
