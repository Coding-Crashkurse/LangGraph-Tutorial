{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen abc>:123: RuntimeWarning: coroutine 'main' was never awaited\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sum of 7 and 22 is 29.\n",
      "The current weather in Tokyo is clear with a temperature of 17Â°C. There is a wind speed of 49 km/h, and no precipitation is expected. Visibility is 10 km.\n",
      "The secret word is \"banana\".\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from agents import Agent, Runner\n",
    "from agents.mcp import MCPServerSse\n",
    "from agents.model_settings import ModelSettings\n",
    "\n",
    "async def main():\n",
    "    server = MCPServerSse(name=\"DemoSSE\", params={\"url\": \"http://localhost:8000/sse\"})\n",
    "    await server.connect()\n",
    "    agent = Agent(\n",
    "        name=\"Assistant\",\n",
    "        instructions=\"Use the tools to answer questions.\",\n",
    "        mcp_servers=[server],\n",
    "        model_settings=ModelSettings(tool_choice=\"required\"),\n",
    "    )\n",
    "\n",
    "    prompt = \"Add these numbers: 7 and 22.\"\n",
    "    result = await Runner.run(starting_agent=agent, input=prompt)\n",
    "    print(result.final_output)\n",
    "\n",
    "    prompt = \"What's the weather in Tokyo?\"\n",
    "    result = await Runner.run(starting_agent=agent, input=prompt)\n",
    "    print(result.final_output)\n",
    "\n",
    "    prompt = \"What's the secret word?\"\n",
    "    result = await Runner.run(starting_agent=agent, input=prompt)\n",
    "    print(result.final_output)\n",
    "\n",
    "    await server.cleanup()\n",
    "\n",
    "await main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import asyncio\n",
    "from typing import Any, List, Optional, Union, Literal\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.utils.function_calling import convert_to_openai_tool\n",
    "\n",
    "class MCPServer:\n",
    "    def __init__(self, cache_tools_list: bool = False):\n",
    "        self.cache_tools_list = cache_tools_list\n",
    "        self._cached_tool_list: list[Any] | None = None\n",
    "\n",
    "    async def list_tools(self) -> list[Any]:\n",
    "        if self.cache_tools_list and self._cached_tool_list is not None:\n",
    "            return self._cached_tool_list\n",
    "        tools = await self._fetch_tools_from_server()\n",
    "        if self.cache_tools_list:\n",
    "            self._cached_tool_list = tools\n",
    "        return tools\n",
    "\n",
    "    def invalidate_tools_cache(self) -> None:\n",
    "        self._cached_tool_list = None\n",
    "\n",
    "    async def _fetch_tools_from_server(self) -> list[Any]:\n",
    "        return []\n",
    "\n",
    "class ChatOpenAIWithServers(ChatOpenAI):\n",
    "    def __init__(self, *args: Any, **kwargs: Any) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self._server_tool_cache: dict[MCPServer, list[dict]] = {}\n",
    "\n",
    "    async def bind_servers(\n",
    "        self,\n",
    "        servers: List[MCPServer],\n",
    "        *,\n",
    "        force_refresh: bool = False,\n",
    "        tool_choice: Union[dict, str, Literal[\"auto\",\"none\",\"required\",\"any\"], bool, None] = None,\n",
    "        strict: Optional[bool] = None,\n",
    "        parallel_tool_calls: Optional[bool] = None,\n",
    "        **kwargs: Any\n",
    "    ) -> ChatOpenAIWithServers:\n",
    "        all_tools = []\n",
    "        for server in servers:\n",
    "            if force_refresh:\n",
    "                server.invalidate_tools_cache()\n",
    "                self._server_tool_cache.pop(server, None)\n",
    "            if server in self._server_tool_cache:\n",
    "                cached_tools = self._server_tool_cache[server]\n",
    "                all_tools.extend(cached_tools)\n",
    "            else:\n",
    "                server_tools = await server.list_tools()\n",
    "                converted = []\n",
    "                for t in server_tools:\n",
    "                    tool_dict = {\n",
    "                        \"name\": getattr(t, \"name\", \"unnamed\"),\n",
    "                        \"description\": getattr(t, \"description\", \"\"),\n",
    "                        \"parameters\": getattr(t, \"inputSchema\", {\"type\": \"object\", \"properties\": {}})\n",
    "                    }\n",
    "                    converted.append(convert_to_openai_tool(tool_dict, strict=strict))\n",
    "                self._server_tool_cache[server] = converted\n",
    "                all_tools.extend(converted)\n",
    "        return self.bind_tools(\n",
    "            all_tools,\n",
    "            tool_choice=tool_choice,\n",
    "            strict=strict,\n",
    "            parallel_tool_calls=parallel_tool_calls,\n",
    "            **kwargs\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents.mcp import MCPServerSse\n",
    "\n",
    "server = MCPServerSse(params={\"url\": \"http://localhost:8000/sse\"})\n",
    "await server.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAIWithServers()\n",
    "llm_with_servers = await llm.bind_servers(servers=[server])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_Hz2Pjnf1CriHPY8OmxJwx3WX', 'function': {'arguments': '{\"city\":\"Munich\"}', 'name': 'get_current_weather'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 97, 'total_tokens': 115, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-BGAXK04VMvaVxLToxNvq4UBMfjHpQ', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-a3f98e2b-54dd-4457-8b61-09909640ed5f-0', tool_calls=[{'name': 'get_current_weather', 'args': {'city': 'Munich'}, 'id': 'call_Hz2Pjnf1CriHPY8OmxJwx3WX', 'type': 'tool_call'}], usage_metadata={'input_tokens': 97, 'output_tokens': 18, 'total_tokens': 115, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
