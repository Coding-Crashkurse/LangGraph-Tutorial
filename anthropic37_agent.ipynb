{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_anthropic.chat_models import ChatAnthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatAnthropic(model_name=\"claude-3-5-haiku-20241022\")\n",
    "llm_reasoning = ChatAnthropic(\n",
    "    model_name=\"claude-3-7-sonnet-latest\",\n",
    "    model_kwargs={\n",
    "        \"max_tokens\": 20000,\n",
    "        \"thinking\": {\"type\": \"enabled\", \"budget_tokens\": 1024},\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = llm_reasoning.invoke(\"What is the root of 12\")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.content[0][\"thinking\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.content[1][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_classifier = ChatAnthropic(model_name=\"claude-3-5-haiku-20241022\")\n",
    "\n",
    "llm_regular = ChatAnthropic(\n",
    "    model_name=\"claude-3-7-sonnet-latest\",\n",
    "    model_kwargs={\"max_tokens\": 3000, \"thinking\": {\"type\": \"disabled\"}},\n",
    ")\n",
    "\n",
    "llm_thinking = ChatAnthropic(\n",
    "    model_name=\"claude-3-7-sonnet-latest\",\n",
    "    model_kwargs={\n",
    "        \"max_tokens_to_sample\": 20000,\n",
    "        \"thinking\": {\"type\": \"enabled\", \"budget_tokens\": 1024},\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph\n",
    "from typing import Literal\n",
    "from typing import TypedDict\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.messages import BaseMessage, HumanMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DifficultyGrade(BaseModel):\n",
    "    \"\"\"Model for capturing difficulty classification.\"\"\"\n",
    "\n",
    "    difficulty: str = Field(description=\"One of: easy, mid, hard, or very hard.\")\n",
    "\n",
    "\n",
    "class DifficultyState(TypedDict):\n",
    "    messages: list[BaseMessage]\n",
    "    difficulty: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_difficulty(state: DifficultyState) -> DifficultyState:\n",
    "    question = state[\"messages\"][-1].content\n",
    "\n",
    "    system_prompt = (\n",
    "        \"You are a difficulty classifier. \"\n",
    "        \"Classify the user question into exactly one of these categories: \"\n",
    "        \"easy, mid, hard, or very hard. Return ONLY the single word: \"\n",
    "        \"easy, mid, hard, or very hard.\"\n",
    "    )\n",
    "    human_prompt = f\"User question: {question}\"\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", system_prompt),\n",
    "            (\"human\", human_prompt),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    structured_llm = llm_classifier.with_structured_output(DifficultyGrade)\n",
    "    chain = prompt | structured_llm\n",
    "\n",
    "    result = chain.invoke({})\n",
    "    state[\"difficulty\"] = result.difficulty.lower().strip()\n",
    "\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def route_based_on_difficulty(state: DifficultyState) -> Literal[\"thinking\", \"regular\"]:\n",
    "    if state[\"difficulty\"] == \"very hard\":\n",
    "        return \"thinking\"\n",
    "    else:\n",
    "        return \"regular\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_model_thinking(state: DifficultyState) -> DifficultyState:\n",
    "    user_prompt = state[\"messages\"][-1].content\n",
    "\n",
    "    ai_response = llm_thinking.invoke([HumanMessage(content=user_prompt)])\n",
    "    state[\"messages\"].append(ai_response)\n",
    "\n",
    "    return state\n",
    "\n",
    "\n",
    "def call_model_regular(state: DifficultyState) -> DifficultyState:\n",
    "    user_prompt = state[\"messages\"][-1].content\n",
    "\n",
    "    ai_response = llm_regular.invoke([HumanMessage(content=user_prompt)])\n",
    "    state[\"messages\"].append(ai_response)\n",
    "\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(DifficultyState)\n",
    "\n",
    "workflow.add_node(\"classify_difficulty\", classify_difficulty)\n",
    "workflow.add_node(\"call_model_thinking\", call_model_thinking)\n",
    "workflow.add_node(\"call_model_regular\", call_model_regular)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.set_entry_point(\"classify_difficulty\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"classify_difficulty\",\n",
    "    route_based_on_difficulty,\n",
    "    {\n",
    "        \"thinking\": \"call_model_thinking\",\n",
    "        \"regular\": \"call_model_regular\",\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"call_model_thinking\", END)\n",
    "workflow.add_edge(\"call_model_regular\", END)\n",
    "\n",
    "\n",
    "graph = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "from langchain_core.runnables.graph import MermaidDrawMethod\n",
    "\n",
    "display(\n",
    "    Image(\n",
    "        graph.get_graph().draw_mermaid_png(\n",
    "            draw_method=MermaidDrawMethod.API,\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.invoke(input={\"messages\": [HumanMessage(content=\"What is 2+2?\")]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = graph.invoke(\n",
    "    input={\n",
    "        \"messages\": [\n",
    "            HumanMessage(\n",
    "                content=\"Could you provide a detailed proof of Fermat's Last Theorem?\"\n",
    "            )\n",
    "        ]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[\"messages\"][1].content[1][\"text\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
