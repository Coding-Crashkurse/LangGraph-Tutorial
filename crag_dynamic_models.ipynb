{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_llm():\n",
    "    llm_type = os.getenv(\"LLM_TYPE\", \"ollama\")\n",
    "    if llm_type == \"ollama\":\n",
    "        return ChatOllama(model=\"llama3.1\", temperature=0)\n",
    "    else:\n",
    "        return ChatOpenAI(temperature=0, model=\"gpt-4o-mini\")\n",
    "\n",
    "def get_embeddings():\n",
    "    embedding_type = os.getenv(\"LLM_TYPE\", \"ollama\")\n",
    "    if embedding_type == \"ollama\":\n",
    "        return OllamaEmbeddings(model=\"llama3.1\")\n",
    "    else:\n",
    "        return OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating VectorDatabase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "embedding_function = get_embeddings()\n",
    "\n",
    "docs = [\n",
    "    Document(\n",
    "        page_content=\"Bella Vista is owned by Antonio Rossi, a renowned chef with over 20 years of experience in the culinary industry. He started Bella Vista to bring authentic Italian flavors to the community.\",\n",
    "        metadata={\"source\": \"restaurant_info.txt\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Bella Vista offers a range of dishes with prices that cater to various budgets. Appetizers start at $8, main courses range from $15 to $35, and desserts are priced between $6 and $12.\",\n",
    "        metadata={\"source\": \"restaurant_info.txt\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Bella Vista is open from Monday to Sunday. Weekday hours are 11:00 AM to 10:00 PM, while weekend hours are extended from 11:00 AM to 11:00 PM.\",\n",
    "        metadata={\"source\": \"restaurant_info.txt\"},\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Bella Vista offers a variety of menus including a lunch menu, dinner menu, and a special weekend brunch menu. The lunch menu features light Italian fare, the dinner menu offers a more extensive selection of traditional and contemporary dishes, and the brunch menu includes both classic breakfast items and Italian specialties.\",\n",
    "        metadata={\"source\": \"restaurant_info.txt\"},\n",
    "    ),\n",
    "]\n",
    "\n",
    "db = Chroma.from_documents(docs, embedding_function)\n",
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict\n",
    "\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    question: str\n",
    "    grades: list[str]\n",
    "    llm_output: str\n",
    "    documents: list[str]\n",
    "    on_topic: bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_docs(state: AgentState):\n",
    "    question = state[\"question\"]\n",
    "    documents = retriever.get_relevant_documents(query=question)\n",
    "    print(\"RETRIEVED DOCUMENTS:\", documents)\n",
    "    state[\"documents\"] = [doc.page_content for doc in documents]\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "class GradeQuestion(BaseModel):\n",
    "    \"\"\"Boolean value to check whether a question is releated to the restaurant Bella Vista\"\"\"\n",
    "\n",
    "    score: str = Field(\n",
    "        description=\"Question is about restaurant? If yes -> 'Yes' if not -> 'No'\"\n",
    "    )\n",
    "\n",
    "\n",
    "def question_classifier(state: AgentState):\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    system = \"\"\"You are a grader assessing the topic a user question. \\n\n",
    "        Only answer if the question is about one of the following topics:\n",
    "        1. Information about the owner of Bella Vista (Antonio Rossi).\n",
    "        2. Prices of dishes at Bella Vista.\n",
    "        3. Opening hours of Bella Vista.\n",
    "        4. Available menus at Bella Vista.\n",
    "\n",
    "        Examples: How will the weather be today -> No\n",
    "                  Who owns the restaurant? -> Yes\n",
    "                  What food do you offer? -> Yes\n",
    "\n",
    "        If the question IS about these topics response with \"Yes\", otherwise respond with \"No\".\n",
    "        \"\"\"\n",
    "\n",
    "    grade_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", system),\n",
    "            (\"human\", \"User question: {question}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    llm = get_llm()\n",
    "    structured_llm = llm.with_structured_output(GradeQuestion)\n",
    "    grader_llm = grade_prompt | structured_llm\n",
    "    result = grader_llm.invoke({\"question\": question})\n",
    "    print(f\"QUESTION and GRADE: {question} - {result.score}\")\n",
    "    state[\"on_topic\"] = result.score\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_topic_router(state: AgentState):\n",
    "    on_topic = state[\"on_topic\"]\n",
    "    if on_topic.lower() == \"yes\":\n",
    "        return \"on_topic\"\n",
    "    return \"off_topic\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def off_topic_response(state: AgentState):\n",
    "    state[\"llm_output\"] = \"I cant respond to that!\"\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradeDocuments(BaseModel):\n",
    "    \"\"\"Boolean values to check for relevance on retrieved documents.\"\"\"\n",
    "\n",
    "    score: str = Field(\n",
    "        description=\"Documents are relevant to the question, 'Yes' or 'No'\"\n",
    "    )\n",
    "\n",
    "\n",
    "def document_grader(state: AgentState):\n",
    "    docs = state[\"documents\"]\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    system = \"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n\n",
    "        If the document contains keyword(s) or semantic meaning related to the question, grade it as relevant. \\n\n",
    "        Give a binary score 'Yes' or 'No' score to indicate whether the document is relevant to the question.\"\"\"\n",
    "\n",
    "    grade_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", system),\n",
    "            (\n",
    "                \"human\",\n",
    "                \"Retrieved document: \\n\\n {document} \\n\\n User question: {question}\",\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    llm = get_llm()\n",
    "    structured_llm = llm.with_structured_output(GradeDocuments)\n",
    "    grader_llm = grade_prompt | structured_llm\n",
    "    scores = []\n",
    "    for doc in docs:\n",
    "        result = grader_llm.invoke({\"document\": doc, \"question\": question})\n",
    "        scores.append(result.score)\n",
    "    state[\"grades\"] = scores\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_router(state: AgentState):\n",
    "    grades = state[\"grades\"]\n",
    "    print(\"DOCUMENT GRADES:\", grades)\n",
    "\n",
    "    if any(grade.lower() == \"yes\" for grade in grades):\n",
    "        filtered_grades = [grade for grade in grades if grade.lower() == \"yes\"]\n",
    "        print(\"FILTERED DOCUMENT GRADES:\", filtered_grades)\n",
    "        return \"generate\"\n",
    "    else:\n",
    "        return \"rewrite_query\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "def rewriter(state: AgentState):\n",
    "    question = state[\"question\"]\n",
    "    system = \"\"\"You a question re-writer that converts an input question to a better version that is optimized \\n\n",
    "        for retrieval. Look at the input and try to reason about the underlying semantic intent / meaning.\"\"\"\n",
    "    re_write_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", system),\n",
    "            (\n",
    "                \"human\",\n",
    "                \"Here is the initial question: \\n\\n {question} \\n Formulate an improved question.\",\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    llm = get_llm()\n",
    "    question_rewriter = re_write_prompt | llm | StrOutputParser()\n",
    "    output = question_rewriter.invoke({\"question\": question})\n",
    "    state[\"question\"] = output\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "\n",
    "def generate_answer(state: AgentState):\n",
    "    llm = get_llm()\n",
    "    question = state[\"question\"]\n",
    "    context = state[\"documents\"]\n",
    "\n",
    "    template = \"\"\"Answer the question based only on the following context:\n",
    "    {context}\n",
    "\n",
    "    Question: {question}\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        template=template,\n",
    "    )\n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "    result = chain.invoke({\"question\": question, \"context\": context})\n",
    "    state[\"llm_output\"] = result\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "workflow.add_node(\"topic_decision\", question_classifier)\n",
    "workflow.add_node(\"off_topic_response\", off_topic_response)\n",
    "workflow.add_node(\"retrieve_docs\", retrieve_docs)\n",
    "workflow.add_node(\"rewrite_query\", rewriter)\n",
    "workflow.add_node(\"generate_answer\", generate_answer)\n",
    "workflow.add_node(\"document_grader\", document_grader)\n",
    "\n",
    "workflow.add_edge(\"off_topic_response\", END)\n",
    "workflow.add_edge(\"retrieve_docs\", \"document_grader\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"topic_decision\",\n",
    "    on_topic_router,\n",
    "    {\n",
    "        \"on_topic\": \"retrieve_docs\",\n",
    "        \"off_topic\": \"off_topic_response\",\n",
    "    },\n",
    ")\n",
    "workflow.add_conditional_edges(\n",
    "    \"document_grader\",\n",
    "    gen_router,\n",
    "    {\n",
    "        \"generate\": \"generate_answer\",\n",
    "        \"rewrite_query\": \"rewrite_query\",\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"rewrite_query\", \"retrieve_docs\")\n",
    "workflow.add_edge(\"generate_answer\", END)\n",
    "\n",
    "\n",
    "workflow.set_entry_point(\"topic_decision\")\n",
    "\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(app.get_graph(xray=True).draw_mermaid_png()))\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = app.invoke({\"question\": \"How is the weather?\"})\n",
    "result[\"llm_output\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = app.invoke({\"question\": \"Who is the owner of bella vista?\"})\n",
    "result[\"llm_output\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "app",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
