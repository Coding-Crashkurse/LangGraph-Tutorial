{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from abc import ABC, abstractmethod\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.tools import Tool\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "from langchain.schema import Document\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_documents(docs: list[\"Document\"]) -> str:\n",
    "    \"\"\"\n",
    "    Summarize the given documents in one to two sentences.\n",
    "    \"\"\"\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "    all_text = \"\\n\".join(doc.page_content for doc in docs)\n",
    "    prompt = f\"Please summarize the following text in 1-2 sentences:\\n---\\n{all_text}\\n---\"\n",
    "    summary = llm.invoke(prompt)\n",
    "    return summary.content.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AbstractVectorStoreObserver(ABC):\n",
    "    \"\"\"\n",
    "    Ein Interface für alle Observer, die benachrichtigt werden möchten,\n",
    "    sobald ein VectorStore neue Dokumente bekommt / aktualisiert wird.\n",
    "    \"\"\"\n",
    "    @abstractmethod\n",
    "    def on_vectorstore_update(self, manager: \"SingleVectorStoreManager\"):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleVectorStoreManager:\n",
    "    def __init__(self, persist_dir: str):\n",
    "        self.embedding_function = OpenAIEmbeddings()\n",
    "        self.persist_dir = persist_dir\n",
    "\n",
    "        collection_name = os.path.basename(persist_dir)\n",
    "        self.vs = Chroma(\n",
    "            collection_name=collection_name,\n",
    "            embedding_function=self.embedding_function,\n",
    "            persist_directory=self.persist_dir\n",
    "        )\n",
    "\n",
    "        self.description = \"Dieser Vectorstore ist leer.\"\n",
    "\n",
    "        self.observers: list[AbstractVectorStoreObserver] = []\n",
    "\n",
    "    def add_observer(self, observer: AbstractVectorStoreObserver):\n",
    "        self.observers.append(observer)\n",
    "\n",
    "    def remove_observer(self, observer: AbstractVectorStoreObserver):\n",
    "        if observer in self.observers:\n",
    "            self.observers.remove(observer)\n",
    "\n",
    "    def notify_observers(self):\n",
    "        for obs in self.observers:\n",
    "            obs.on_vectorstore_update(self)\n",
    "\n",
    "    def is_empty(self) -> bool:\n",
    "        return (self.vs._collection.count() == 0)\n",
    "\n",
    "    def create_retriever_tool(self, name: str, custom_description: str | None = None) -> Tool:\n",
    "\n",
    "        retriever = self.vs.as_retriever()\n",
    "        desc = custom_description if custom_description else self.description\n",
    "        if self.is_empty():\n",
    "            desc += \"\\n(Hinweis: Dieser Vectorstore ist aktuell leer.)\"\n",
    "\n",
    "        tool = create_retriever_tool(\n",
    "            retriever=retriever,\n",
    "            name=name,\n",
    "            description=desc\n",
    "        )\n",
    "        return tool\n",
    "\n",
    "    def add_documents(self, docs: list[Document], update_description: bool = True):\n",
    "\n",
    "        self.vs.add_documents(docs)\n",
    "        if update_description:\n",
    "            summary_text = summarize_documents(docs)\n",
    "            if self.is_empty():\n",
    "                pass\n",
    "            self.description = summary_text\n",
    "        self.notify_observers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import Tool\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, ToolMessage\n",
    "\n",
    "class LLMToolBinder:\n",
    "    def __init__(self, llm_with_tools: ChatOpenAI, managers: list[\"SingleVectorStoreManager\"], extra_tools: list[Tool] | None = None):\n",
    "        self.llm_with_tools = llm_with_tools\n",
    "        self.llm_no_tools = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "        self.managers = managers\n",
    "        self.extra_tools = extra_tools or []\n",
    "        self.tools: list[Tool] = []\n",
    "        self._bind_tools()\n",
    "\n",
    "    def _bind_tools(self):\n",
    "        new_tools = []\n",
    "        for i, m in enumerate(self.managers, start=1):\n",
    "            tool_name = f\"retriever_store{i}\"\n",
    "            new_tools.append(m.create_retriever_tool(name=tool_name))\n",
    "        new_tools.extend(self.extra_tools)\n",
    "        self.tools = new_tools\n",
    "        self.llm_with_tools = self.llm_with_tools.bind_tools(self.tools, tool_choice=\"required\")\n",
    "\n",
    "    def on_vectorstore_update(self, manager: \"SingleVectorStoreManager\"):\n",
    "        self._bind_tools()\n",
    "\n",
    "    def invoke_llm(self, query: str) -> str:\n",
    "        system_prompt = (\n",
    "            \"You are a helpful assistant. You may call the available tools if needed. \"\n",
    "            \"Once you receive tool outputs, focus on the last tool message and provide a final user-facing answer.\"\n",
    "        )\n",
    "        messages = [SystemMessage(content=system_prompt), HumanMessage(content=query)]\n",
    "        first_output = self.llm_with_tools.invoke(messages)\n",
    "        messages.append(first_output)\n",
    "        if first_output.tool_calls:\n",
    "            for tc in first_output.tool_calls:\n",
    "                tool_name = tc[\"name\"]\n",
    "                tool_args = tc[\"args\"]\n",
    "                print(f\"Tool chosen: {tool_name} with args={tool_args}\")\n",
    "                found_tool = next((t for t in self.tools if t.name.lower() == tool_name.lower()), None)\n",
    "                if not found_tool:\n",
    "                    tool_result = f\"No matching tool named '{tool_name}'.\"\n",
    "                else:\n",
    "                    tool_result = found_tool.invoke(tool_args)\n",
    "                messages.append(ToolMessage(content=tool_result, tool_call_id=tc[\"id\"]))\n",
    "            messages.append(SystemMessage(content=\"Focus on the last tool message. Provide your final answer.\"))\n",
    "            second_output = self.llm_no_tools.invoke(messages)\n",
    "            messages.append(second_output)\n",
    "            return second_output.content\n",
    "        else:\n",
    "            return first_output.content\n",
    "\n",
    "    def print_all_tool_descriptions(self):\n",
    "        for tool in self.tools:\n",
    "            print(tool.name, \":\", tool.description)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"my_chroma_db\"\n",
    "os.makedirs(base_dir, exist_ok=True)\n",
    "\n",
    "manager1 = SingleVectorStoreManager(os.path.join(base_dir, \"store1\"))\n",
    "manager2 = SingleVectorStoreManager(os.path.join(base_dir, \"store2\"))\n",
    "manager3 = SingleVectorStoreManager(os.path.join(base_dir, \"store3\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def fallback_tool(message: str) -> str:\n",
    "    \"\"\"\n",
    "    A fallback tool if no other tool is appropriate.\n",
    "\n",
    "    Args:\n",
    "        message (str): The user query, or any text.\n",
    "\n",
    "    Returns:\n",
    "        str: A fallback response for questions that the model cannot answer\n",
    "             with the other tools.\n",
    "    \"\"\"\n",
    "    return f\"I don´t know how to answer {message}'\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "def create_vectorstore_info_tool(managers: list[\"SingleVectorStoreManager\"]):\n",
    "    @tool\n",
    "    def vectorstore_info(query: str) -> str:\n",
    "        \"\"\"\n",
    "        Use this tool to reveal internal knowledge about the agent, including:\n",
    "        - The total number of vectorstores,\n",
    "        - Each vectorstore’s document count,\n",
    "        - Each vectorstore’s description or summary.\n",
    "        \"\"\"\n",
    "        lines = [f\"Total vectorstores: {len(managers)}\"]\n",
    "        for i, m in enumerate(managers, start=1):\n",
    "            doc_count = m.vs._collection.count()\n",
    "            lines.append(\n",
    "                f\"VectorStore {i}: {doc_count} documents\\n\"\n",
    "                f\"Description: {m.description}\"\n",
    "            )\n",
    "        return \"\\n\\n\".join(lines)\n",
    "\n",
    "    return vectorstore_info\n",
    "\n",
    "\n",
    "info_tool = create_vectorstore_info_tool(managers=[manager1, manager2, manager3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "binder = LLMToolBinder(llm, [manager1, manager2, manager3], extra_tools=[fallback_tool, info_tool])\n",
    "\n",
    "manager1.add_observer(binder)\n",
    "manager2.add_observer(binder)\n",
    "manager3.add_observer(binder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binder.invoke_llm(\"Where is Lacarelli?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_store1 = [\n",
    "    Document(\n",
    "        page_content=(\n",
    "            \"Lacarelli is a charming family-run Italian restaurant nestled in the \"\n",
    "            \"heart of Berlin. Its menu features authentic dishes like homemade \"\n",
    "            \"ravioli, wood-fired pizzas, and creamy tiramisu. With friendly staff, \"\n",
    "            \"rustic decor, and a cozy atmosphere, Lacarelli provides an inviting \"\n",
    "            \"dining experience for lovers of Italian cuisine and fine wines daily.\"\n",
    "        )\n",
    "    )\n",
    "]\n",
    "manager1.add_documents(docs_store1, update_description=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binder.print_all_tool_descriptions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binder.invoke_llm(\"Where is Lacarelli?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binder.invoke_llm(\"What do you know?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binder.invoke_llm(\"How many vectorstores do you manage?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
